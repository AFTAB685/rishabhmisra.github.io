---
layout: post
type: blog
title: <center>Machine Learning Glossary</center>
comments: true
mathjax: true
---

## Introduction
The goal of this post is to briefly explain popular (and unpopular) concepts in Machine Learning along with a collection of helpful links for further understanding. The idea for this post stemmed from my travails for finding good quality explanations of various Machine Learning concepts on the web. Hopefully, this post would be helpful to people who are just starting in Machine Learning as well as to people who need a quick refresher on some concepts. 

**Didn't find what you were looking for? Consider contributing by creating a pull request on this post [here](https://github.com/rishabhmisra/rishabhmisra.github.io/blob/master/_posts/2018-12-12-Machine-Learning-Glossary.md)**.

## Jump to
[A](#A) . [B](#B) . [C](#C) . [D](#D) . [E](#E) . [F](#F) . [G](#G) . [H](#H) . [I](#I) . [J](#J) . [K](#K) . [L](#L) . [M](#M) . [N](#N) . [O](#O) . [P](#P) . [Q](#Q) . [R](#R) . [S](#S) . [T](#T) . [U](#U) . [V](#V) . [W](#W) . [X](#X) . [Y](#Y) . [Z](#Z)

## A<a name="A"></a>
* **AUC**<a name="AUC"></a>: AUC is the **A**rea **U**nder the Receiver Operating Characteristic (ROC) **C**urve. ROC curve is obtained by varying the classification threshold of a binary classifier and plotting the true positive rate ([TPR](#TPR)) against the false positive rate ([FPR](#FPR)) at each threhold. It is a popular classification performance metric and has several nice properties like being independent of decision threshold, being robust to class imbalance in data and so on.
  * Useful links: [Video Explanation of AUC](https://www.youtube.com/watch?v=OAl6eAyP-yo) | [Probabilistic interpretation of AUC](https://www.alexejgossmann.com/auc/)

## B<a name="B"></a>

## C<a name="C"></a>

## D<a name="D"></a>

## E<a name="E"></a>

## F<a name="F"></a>
* **False Positive Rate**<a name="FPR"></a>: The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positives) and the total number of actual negative events (regardless of classification).
  * Useful links: [False Positive Rate Wiki](https://en.wikipedia.org/wiki/False_positive_rate)

## G<a name="G"></a>

## H<a name="H"></a>

## I<a name="I"></a>

## J<a name="J"></a>

## K<a name="K"></a>

## L<a name="L"></a>

## M<a name="M"></a>

## N<a name="N"></a>

## O<a name="O"></a>

## P<a name="P"></a>
* **Precision**<a name="Precision"></a>: If we are given a set of instances, precision is the fraction of relevant instances (those correctly classified into a certain class $C$) among the retrieved instances (those belonging to a certain class $C$). A perfect precision score of 1.0 means that every result retrieved by a search was relevant, but says nothing about whether all relevant documents were retrieved.
  * Useful links: [Blog post on Precision and Recall](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c) | [Precision and Recall Wiki](https://en.wikipedia.org/wiki/Precision_and_recall)
 
## Q<a name="Q"></a>

## R<a name="R"></a>
* **Recall**<a name="Recall"></a>: If we are given a set of instances, recall is the fraction of relevant instances (belonging to a certain class) that have been retrieved (or correctly classified) over the total number of relevant instances. A recall of 1.0 means that every item from class $C$ was labeled as belonging to class $C$, but does not say anything about other items that were incorrectly labeled as belonging to class $C$.
  * Useful links: [Blog post on Precision and Recall](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c) | [Precision and Recall Wiki](https://en.wikipedia.org/wiki/Precision_and_recall)

## S<a name="S"></a>
* **Sensitivity**<a name="Sensitivity"></a>: Same as [Recall](#Recall).

## T<a name="T"></a>
* **True Positive Rate**<a name="TPR"></a>: Same as [Recall](#Recall).

## U<a name="U"></a>

## V<a name="V"></a>

## W<a name="W"></a>

## X<a name="X"></a>

## Y<a name="Y"></a>

## Z<a name="Z"></a>
